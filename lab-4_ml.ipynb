{"cells":[{"cell_type":"markdown","metadata":{"id":"hGClrhQA9SAk"},"source":["# Деревья решений"]},{"cell_type":"markdown","metadata":{"id":"veekMy8WRjBi"},"source":["## Построение дерева"]},{"cell_type":"markdown","metadata":{"id":"SYkVwAFiUHXj"},"source":["Опишем жадный алгоритм построения бинарного дерева решений:\n","1. Начинаем со всей обучающей выборки $X$, которую помещаем в корень $R_1$.\n","2. Задаём функционал качества $Q(X, j, t)$ и критерий остановки.\n","3. Запускаем построение из корня: $SplitNode(1, R_1)$\n","\n","Функция $SplitNode(m, R_m)$\n","1. Если выполнен критерий остановки, то выход.\n","2. Находим наилучший с точки зрения $Q$ предикат: $j, t$: $[x_j<t]$\n","3. Помещаем предикат в вершину и получаем с его помощью разбиение $X$ на две части: $R_{left} = \\lbrace x|x_j<t \\rbrace$ и $R_{right} = \\lbrace x|x_j \\geqslant t \\rbrace$\n","4. Поместим $R_{left}$ и $R_{right}$ соответсвенно в левое и правое поддерево.\n","5. Рекурсивно повторяем $SplitNode(left, R_{left})$ и $SplitNode(right, R_{right})$.\n","\n","В конце поставим в соответствие каждому листу ответ. Для задачи классификации - это самый частый среди объектов класс или вектор с долями классов (можно интерпретировать как вероятности):\n","$$ c_v = \\arg \\max_{k\\in Y} \\sum_{(x_i,y_i) \\in R_v} [y_i=k]  $$"]},{"cell_type":"markdown","metadata":{"id":"9P6FsdBog4Ai"},"source":["## Функционал качества для деревьев решений\n"]},{"cell_type":"markdown","metadata":{"id":"9VAKO0aykGBD"},"source":["Энтропия Шеннона для системы с N возможными состояниями определяется по формуле:\n","$$H = - \\sum_{i=0}^{N} p_i\\log_2p_i $$"]},{"cell_type":"markdown","metadata":{"id":"5582B-1Fn2bw"},"source":["где $p_i$ – вероятности нахождения системы в $i$-ом состоянии.\n","\n","Это очень важное понятие теории информации, которое позволяет оценить количество информации (степень хаоса в системе). Чем выше энтропия, тем менее упорядочена система и наоборот. С помощью энтропии мы формализуем функционал качества для разделение выборки (для задачи классификации)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PbcMUd7bvk05"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","import random\n","from pprint import pprint"]},{"cell_type":"markdown","metadata":{"id":"4AdLxP9CowTm"},"source":["Код для расчёта энтропии:"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"2mT8Jq8Av2sM"},"outputs":[],"source":["def entropy(y : pd.Series):\n","\n","    _, counts = np.unique(y, return_counts=True)\n","\n","    probabilities = counts / counts.sum()\n","    entropy = sum(probabilities * -np.log2(probabilities))\n","\n","    return entropy"]},{"cell_type":"markdown","metadata":{"id":"Xk9etb2vo7fK"},"source":["Здесь $y$ - это массив значений целевой переменной"]},{"cell_type":"markdown","metadata":{"id":"07TCw0USzLus"},"source":["Энтропия – по сути степень хаоса (или неопределенности) в системе. Уменьшение энтропии называют приростом информации (information gain, IG).\n","\n","Обочначим $R_v$ - объекты, которые нужно разделить в помощью предиката в вершине $v$. Запишем формулу для расчёта информационного прироста:\n","$$ Q = IG = H(R_v) - (H(R_{left})+H(R_{right}))$$\n","\n","На каждом шаге нам нужно максимизировать этот функционал качества. Как это делать? Например, так можно перебрать $t$ для выбранного $j$."]},{"cell_type":"markdown","metadata":{"id":"trEWHDoXg_p9"},"source":["Предыдущая версия формулы прироста информации слишком упрощена. В работе необходимо использовать более устойчивую формулу, которая учитывает не только энтропию подмножеств, но и их размер.\n","\n","$$ Q = IG = H(R_v) - \\Big (\\frac{|R_{left}|} {|R_{v}|} H(R_{left})+ \\frac{|R_{right}|} {|R_{v}|} H(R_{right})\\Big)$$\n","\n","где, $|R_{v}|$, $|R_{left}|$ и $|R_{right}|$ - количество элементов в соответствующих множествах."]},{"cell_type":"markdown","metadata":{"id":"9xmN6V_N1xBr"},"source":["\n","### Задание 4.1"]},{"cell_type":"markdown","metadata":{"id":"nWFHZScF2CBF"},"source":["Реализуйте алгоритм построения дерева. Должны быть отдельные функции (методы) для расчёта энтропии (уже есть), для разделения узлов дерева (используйте, например, `pandas`), для подсчёта функционала качества $IG$, для выбора наилучшего разделения (с учетом признаков и порогов), для проверки критерия остановки.\n","\n","Для набора данных `iris` реализуйте алгоритм и минимум три разных критерия остановки из перечисленных ниже:\n","* максимальной глубины дерева = 5\n","* минимального числа объектов в листе = 5\n","* максимальное количество листьев в дереве = 5\n","* purity (остановка, если все объекты в листе относятся к одному классу)\n","\n","Реализуйте функцию `predict` (на вход функции подаётся датафрейм с объектами)\n","\n","Оцените точность каждой модели с помощью метрики доля правильных ответов (`from sklearn.metrics import accuracy_score` или реализовать свою)."]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["from numbers import Number\n","from sklearn.datasets import load_iris\n","\n","def split_df(df : pd.DataFrame, j : int, t : Number):\n","    return df[df[df.columns[j]] < t], df[df[df.columns[j]] >= t]\n","\n","def calc_IG(R_v : pd.DataFrame, R_left : pd.DataFrame, R_right : pd.DataFrame):\n","    return entropy(R_v[R_v.columns[-1]]) - (\n","        R_left.shape[0] / R_v.shape[0] * entropy(R_left[R_left.columns[-1]]) + \n","        R_right.shape[0] / R_v.shape[0] * entropy(R_right[R_right.columns[-1]])\n","    )\n","\n","def get_best_split(R_v : pd.DataFrame):\n","    best_j, best_t, max_IG = 0, 0.0, 0.0\n","    for j in range(len(R_v.columns) - 1):\n","        for t in range(len(R_v)):\n","            left, right = split_df(R_v, j, t)\n","            IG = calc_IG(R_v, left, right)\n","            if IG > max_IG:\n","                best_j = j\n","                best_t = t\n","                max_IG = IG\n","    left, right = split_df(R_v, best_j, best_t)\n","    return {'col': best_j, 't': best_t, 'left': left, 'right': right}\n","\n","def is_pure(R_v : pd.DataFrame):\n","    return R_v[R_v.columns[-1]].value_counts().size == 1\n","\n","def to_leaf(df : pd.DataFrame):\n","    return df[df.columns[-1]].value_counts().index[0]\n","\n","def split_node(depth, parent, max_depth, min_obj):\n","    left = parent['left']\n","    right = parent['right']\n","\n","    if left.empty or right.empty:\n","        parent['left'] = parent['right'] = to_leaf(pd.concat([left, right]))\n","        return\n","\n","    if depth >= max_depth:\n","        parent['left'] = to_leaf(left)\n","        parent['right'] = to_leaf(right)\n","        return\n","\n","    if is_pure(left) or len(left) <= min_obj:\n","        parent['left'] = to_leaf(left)\n","    else:\n","        parent['left'] = get_best_split(left)\n","        split_node(depth + 1, parent['left'], max_depth, min_obj)\n","\n","    if is_pure(right) or len(right) <= min_obj:\n","        parent['right'] = to_leaf(right)\n","    else:\n","        parent['right'] = get_best_split(right)\n","        split_node(depth + 1, parent['right'], max_depth, min_obj)\n","\n","def grow_tree(df, max_depth, min_obj):\n","    root = get_best_split(df)\n","    split_node(1, root, max_depth, min_obj)\n","    return root\n","\n","def predict_row(node, df_row : pd.Series):\n","    if df_row.values[node['col']] < node['t']:\n","        if isinstance(node['left'], dict):\n","            return predict_row(node['left'], df_row)\n","        else:\n","            return node['left']\n","    else:\n","        if isinstance(node['right'], dict):\n","            return predict_row(node['right'], df_row)\n","        else:\n","            return node['right']\n","\n","def predict(root, df):\n","    y_pred = []\n","    for i, row in df.iterrows():\n","        y_pred.append(predict_row(root, row))\n","    return y_pred\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal length (cm)</th>\n","      <th>sepal width (cm)</th>\n","      <th>petal length (cm)</th>\n","      <th>petal width (cm)</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>6.7</td>\n","      <td>3.0</td>\n","      <td>5.2</td>\n","      <td>2.3</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>6.3</td>\n","      <td>2.5</td>\n","      <td>5.0</td>\n","      <td>1.9</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>6.5</td>\n","      <td>3.0</td>\n","      <td>5.2</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>6.2</td>\n","      <td>3.4</td>\n","      <td>5.4</td>\n","      <td>2.3</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>5.9</td>\n","      <td>3.0</td>\n","      <td>5.1</td>\n","      <td>1.8</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 5 columns</p>\n","</div>"],"text/plain":["     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n","0                  5.1               3.5                1.4               0.2   \n","1                  4.9               3.0                1.4               0.2   \n","2                  4.7               3.2                1.3               0.2   \n","3                  4.6               3.1                1.5               0.2   \n","4                  5.0               3.6                1.4               0.2   \n","..                 ...               ...                ...               ...   \n","145                6.7               3.0                5.2               2.3   \n","146                6.3               2.5                5.0               1.9   \n","147                6.5               3.0                5.2               2.0   \n","148                6.2               3.4                5.4               2.3   \n","149                5.9               3.0                5.1               1.8   \n","\n","     target  \n","0       0.0  \n","1       0.0  \n","2       0.0  \n","3       0.0  \n","4       0.0  \n","..      ...  \n","145     2.0  \n","146     2.0  \n","147     2.0  \n","148     2.0  \n","149     2.0  \n","\n","[150 rows x 5 columns]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["df = load_iris()\n","data_full = pd.DataFrame(data=np.c_[df['data'], df['target']], columns=df['feature_names'] + ['target'])\n","data_full"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"data":{"text/plain":["0.94"]},"execution_count":186,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","\n","seed = random.randint(1, 1000)\n","df_train = data_full.sample(100, random_state=seed)\n","df_test = data_full.drop(df_train.index)\n","df_train.reset_index(drop=True, inplace=True)\n","df_test.reset_index(drop=True, inplace=True)\n","\n","root = grow_tree(df_train, 5, 5)\n","accuracy_score(df_test[df_test.columns[-1]], predict(root, df_test))"]},{"cell_type":"markdown","metadata":{"id":"BkyCjLcy_CTM"},"source":["##  Случайный лес"]},{"cell_type":"markdown","metadata":{"id":"7fKZe1FyRgCa"},"source":["Опишем алгоритм случайный лес (*random forest*) и попутно разберём основные идеи:\n","\n","1. Зададим $N$ - число деревьев в лесу.\n","2. Для каждого $n$ из $N$ сгенерируем свою выборку $X_n$. Пусть $m$ - это количество объектов в $X$. При генерации каждой $X_n$ мы будем брать объекты $m$ раз с возвращением. То есть один и тот же объект может попасть в выборку несколько раз, а какие-то объекты не попадут. (Этот способ назвается бутстрап).\n","3. По каждой $X_n$ построим решающее дерево $b_n$. Обычно стараются делать глубокие деревья. В качестве критериев остановки можно использовать `max_depth` или `min_samples_leaf` (например, пока в каждом листе не окажется по одному объекту). При каждом разбиении сначала выбирается $k$ (эвристика $k = \\sqrt d$, где $d$ - это число признаков объектов из выборки $X$) случайных признаков из исходных, и оптимальное разделение выборки ищется только среди них. Обратите внимание, что мы не выбрасываем оставшиеся признаки!\n","4. Итоговый алгоритм будет представлять собой результат голосования (для классификации) и среднее арифметическое (для регрессии). Модификация алгоритма предполагает учёт весов каждого отдельного слабого алгоритма в ансамбле, но в этом особо нет смысла.\n"]},{"cell_type":"markdown","metadata":{"id":"YJBQ8lc0WyrN"},"source":["### Задание 4.2"]},{"cell_type":"markdown","metadata":{"id":"y594Jn04ZTCm"},"source":["В качестве набора данных используйте: https://www.kaggle.com/mathchi/churn-for-bank-customers\n","\n","Там есть описание и примеры работы с этими данными. Если кратко, речь идёт про задачу прогнозирования оттока клиентов. Есть данные о 10 тысячах клиентов банка, часть из которых больше не являются клиентами."]},{"cell_type":"markdown","metadata":{"id":"be_mLbdVW2oG"},"source":["Используя либо свою реализацию, либо  `DecisionTreeClassifier` с разными настройками из `sklearn.tree` реализйте алгоритм \"случайный лес\".\n","\n","Найдите наилучшие гиперпараметры этого алгоритма: количество деревьев, критерий остановки, функционал качества, минимальное количество объектов в листьях и другие.\n","\n","Нельзя использовать готовую реализацию случайного леса из `sklearn`.\n","\n","В подобных задачах очень важна интерпретируемость алгоритма. Попытайтесь оценить информативность признаков, т.е. ответить а вопрос, значения каких признаков являются самыми важными индикаторами того, что банк потеряет клиента."]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting category_encoders\n","  Downloading category_encoders-2.6.2-py2.py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: numpy>=1.14.0 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from category_encoders) (1.25.2)\n","Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from category_encoders) (1.2.2)\n","Requirement already satisfied: scipy>=1.0.0 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from category_encoders) (1.11.1)\n","Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from category_encoders) (0.14.0)\n","Requirement already satisfied: pandas>=1.0.5 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from category_encoders) (2.0.3)\n","Requirement already satisfied: patsy>=0.5.1 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from category_encoders) (0.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n","Requirement already satisfied: six in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n","Requirement already satisfied: packaging>=21.3 in c:\\users\\moroz\\anaconda3\\envs\\py_311\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n","Downloading category_encoders-2.6.2-py2.py3-none-any.whl (81 kB)\n","   ---------------------------------------- 0.0/81.8 kB ? eta -:--:--\n","   ----- ---------------------------------- 10.2/81.8 kB ? eta -:--:--\n","   --------------- ------------------------ 30.7/81.8 kB 435.7 kB/s eta 0:00:01\n","   -------------------- ------------------- 41.0/81.8 kB 487.6 kB/s eta 0:00:01\n","   ---------------------------------------- 81.8/81.8 kB 761.6 kB/s eta 0:00:00\n","Installing collected packages: category_encoders\n","Successfully installed category_encoders-2.6.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install category_encoders"]},{"cell_type":"code","execution_count":222,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RowNumber</th>\n","      <th>CustomerId</th>\n","      <th>Surname</th>\n","      <th>CreditScore</th>\n","      <th>Geography</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>15634602</td>\n","      <td>Hargrave</td>\n","      <td>619</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>42</td>\n","      <td>2</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>101348.88</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>15647311</td>\n","      <td>Hill</td>\n","      <td>608</td>\n","      <td>Spain</td>\n","      <td>Female</td>\n","      <td>41</td>\n","      <td>1</td>\n","      <td>83807.86</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>112542.58</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>15619304</td>\n","      <td>Onio</td>\n","      <td>502</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>42</td>\n","      <td>8</td>\n","      <td>159660.80</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113931.57</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>15701354</td>\n","      <td>Boni</td>\n","      <td>699</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>39</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>93826.63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>15737888</td>\n","      <td>Mitchell</td>\n","      <td>850</td>\n","      <td>Spain</td>\n","      <td>Female</td>\n","      <td>43</td>\n","      <td>2</td>\n","      <td>125510.82</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>79084.10</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>9996</td>\n","      <td>15606229</td>\n","      <td>Obijiaku</td>\n","      <td>771</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>39</td>\n","      <td>5</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>96270.64</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9997</td>\n","      <td>15569892</td>\n","      <td>Johnstone</td>\n","      <td>516</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>35</td>\n","      <td>10</td>\n","      <td>57369.61</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>101699.77</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>9998</td>\n","      <td>15584532</td>\n","      <td>Liu</td>\n","      <td>709</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>36</td>\n","      <td>7</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>42085.58</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>9999</td>\n","      <td>15682355</td>\n","      <td>Sabbatini</td>\n","      <td>772</td>\n","      <td>Germany</td>\n","      <td>Male</td>\n","      <td>42</td>\n","      <td>3</td>\n","      <td>75075.31</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>92888.52</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>10000</td>\n","      <td>15628319</td>\n","      <td>Walker</td>\n","      <td>792</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>28</td>\n","      <td>4</td>\n","      <td>130142.79</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>38190.78</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 14 columns</p>\n","</div>"],"text/plain":["      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n","0             1    15634602   Hargrave          619    France  Female   42   \n","1             2    15647311       Hill          608     Spain  Female   41   \n","2             3    15619304       Onio          502    France  Female   42   \n","3             4    15701354       Boni          699    France  Female   39   \n","4             5    15737888   Mitchell          850     Spain  Female   43   \n","...         ...         ...        ...          ...       ...     ...  ...   \n","9995       9996    15606229   Obijiaku          771    France    Male   39   \n","9996       9997    15569892  Johnstone          516    France    Male   35   \n","9997       9998    15584532        Liu          709    France  Female   36   \n","9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n","9999      10000    15628319     Walker          792    France  Female   28   \n","\n","      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n","0          2       0.00              1          1               1   \n","1          1   83807.86              1          0               1   \n","2          8  159660.80              3          1               0   \n","3          1       0.00              2          0               0   \n","4          2  125510.82              1          1               1   \n","...      ...        ...            ...        ...             ...   \n","9995       5       0.00              2          1               0   \n","9996      10   57369.61              1          1               1   \n","9997       7       0.00              1          0               1   \n","9998       3   75075.31              2          1               0   \n","9999       4  130142.79              1          1               0   \n","\n","      EstimatedSalary  Exited  \n","0           101348.88       1  \n","1           112542.58       0  \n","2           113931.57       1  \n","3            93826.63       0  \n","4            79084.10       0  \n","...               ...     ...  \n","9995         96270.64       0  \n","9996        101699.77       0  \n","9997         42085.58       1  \n","9998         92888.52       1  \n","9999         38190.78       0  \n","\n","[10000 rows x 14 columns]"]},"execution_count":222,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('tables/churn.csv')\n","data"]},{"cell_type":"code","execution_count":223,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditScore</th>\n","      <th>Geography</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.326221</td>\n","      <td>0.161548</td>\n","      <td>0</td>\n","      <td>0.293517</td>\n","      <td>2</td>\n","      <td>-1.225848</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.021886</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.440036</td>\n","      <td>0.166734</td>\n","      <td>0</td>\n","      <td>0.198164</td>\n","      <td>1</td>\n","      <td>0.117350</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.216534</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.536794</td>\n","      <td>0.161548</td>\n","      <td>0</td>\n","      <td>0.293517</td>\n","      <td>8</td>\n","      <td>1.333053</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.240687</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.501521</td>\n","      <td>0.161548</td>\n","      <td>0</td>\n","      <td>0.007457</td>\n","      <td>1</td>\n","      <td>-1.225848</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.108918</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.063884</td>\n","      <td>0.166734</td>\n","      <td>0</td>\n","      <td>0.388871</td>\n","      <td>2</td>\n","      <td>0.785728</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>-0.365276</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>1.246488</td>\n","      <td>0.161548</td>\n","      <td>1</td>\n","      <td>0.007457</td>\n","      <td>5</td>\n","      <td>-1.225848</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>-0.066419</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>-1.391939</td>\n","      <td>0.161548</td>\n","      <td>1</td>\n","      <td>-0.373958</td>\n","      <td>10</td>\n","      <td>-0.306379</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.027988</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>0.604988</td>\n","      <td>0.161548</td>\n","      <td>0</td>\n","      <td>-0.278604</td>\n","      <td>7</td>\n","      <td>-1.225848</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1.008643</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>1.256835</td>\n","      <td>0.324432</td>\n","      <td>1</td>\n","      <td>0.293517</td>\n","      <td>3</td>\n","      <td>-0.022608</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>-0.125231</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>1.463771</td>\n","      <td>0.161548</td>\n","      <td>0</td>\n","      <td>-1.041433</td>\n","      <td>4</td>\n","      <td>0.859965</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>-1.076370</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 11 columns</p>\n","</div>"],"text/plain":["      CreditScore  Geography  Gender       Age  Tenure   Balance  \\\n","0       -0.326221   0.161548       0  0.293517       2 -1.225848   \n","1       -0.440036   0.166734       0  0.198164       1  0.117350   \n","2       -1.536794   0.161548       0  0.293517       8  1.333053   \n","3        0.501521   0.161548       0  0.007457       1 -1.225848   \n","4        2.063884   0.166734       0  0.388871       2  0.785728   \n","...           ...        ...     ...       ...     ...       ...   \n","9995     1.246488   0.161548       1  0.007457       5 -1.225848   \n","9996    -1.391939   0.161548       1 -0.373958      10 -0.306379   \n","9997     0.604988   0.161548       0 -0.278604       7 -1.225848   \n","9998     1.256835   0.324432       1  0.293517       3 -0.022608   \n","9999     1.463771   0.161548       0 -1.041433       4  0.859965   \n","\n","      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n","0                 1          1               1         0.021886       1  \n","1                 1          0               1         0.216534       0  \n","2                 3          1               0         0.240687       1  \n","3                 2          0               0        -0.108918       0  \n","4                 1          1               1        -0.365276       0  \n","...             ...        ...             ...              ...     ...  \n","9995              2          1               0        -0.066419       0  \n","9996              1          1               1         0.027988       0  \n","9997              1          0               1        -1.008643       1  \n","9998              2          1               0        -0.125231       1  \n","9999              1          1               0        -1.076370       0  \n","\n","[10000 rows x 11 columns]"]},"execution_count":223,"metadata":{},"output_type":"execute_result"}],"source":["from category_encoders import TargetEncoder\n","from sklearn.preprocessing import StandardScaler\n","\n","data_target = data['Exited']\n","data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\n","data['Gender'].replace(['Male', 'Female'], [1, 0], inplace=True)\n","data['Geography'] = TargetEncoder().fit_transform(data['Geography'], data_target)\n","data[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']] = StandardScaler().fit_transform(data[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']])\n","data"]},{"cell_type":"code","execution_count":323,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","\n","def grow_forest(X, y, n_trees=100, max_depth=20, min_samples=5):\n","    m = len(X)\n","    k = int(np.sqrt(len(X.columns)))\n","    trees = []\n","    for _ in range(n_trees):\n","        seed = random.randint(1, 1000)\n","        X_sample = X.sample(n=m, random_state=seed, replace=True)\n","        y_sample = y.sample(n=m, random_state=seed, replace=True)\n","        tree = DecisionTreeClassifier(max_depth=max_depth, max_features=k, min_samples_leaf=min_samples)\n","        tree.fit(X_sample, y_sample)\n","        trees.append(tree)\n","    return trees\n","\n","def predict_forest(forest, X):\n","    votes = np.array([tree.predict(X) for tree in forest]).swapaxes(0, 1)\n","    return np.array([np.bincount(row).argmax() for row in votes])\n","\n","def get_best_hyperparams(X_train, y_train, X_test, y_test):\n","    best_tree_cnt = best_depth = best_samples = 0\n","    best_score = -1\n","    for tree_cnt in [100, 200, 300]:\n","        for depth in range(1, 52, 10):\n","            for samples_cnt in range(1, 4):\n","                forest = grow_forest(X_train, y_train, tree_cnt, depth, samples_cnt)\n","                y_pred = predict_forest(forest, X_test)\n","                score = accuracy_score(y_test, y_pred)\n","                if (score > best_score):\n","                    best_tree_cnt, best_depth, best_samples = tree_cnt, depth, samples_cnt\n","    return best_tree_cnt, best_depth, best_samples\n","\n","def feature_importances(forest):\n","    votes = []\n","    for tree in forest:\n","        votes.append(tree.feature_importances_)\n","    votes = pd.DataFrame(votes)\n","    return votes.mean()"]},{"cell_type":"code","execution_count":313,"metadata":{},"outputs":[],"source":["data_X = data.drop('Exited', axis=1)\n","data_y = data['Exited']\n","\n","X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, random_state=17)\n","\n","forest = grow_forest(X_train, y_train)"]},{"cell_type":"code","execution_count":314,"metadata":{},"outputs":[{"data":{"text/plain":["0.8592"]},"execution_count":314,"metadata":{},"output_type":"execute_result"}],"source":["y_pred = predict_forest(forest, X_test)\n","accuracy_score(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_params = get_best_hyperparams(X_train, y_train, X_test, y_test)\n","best_forest = grow_forest(X_train, y_train, *best_params)"]},{"cell_type":"code","execution_count":322,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature</th>\n","      <th>Importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>Age</td>\n","      <td>0.264004</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>NumOfProducts</td>\n","      <td>0.147922</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Balance</td>\n","      <td>0.134915</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>EstimatedSalary</td>\n","      <td>0.124233</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CreditScore</td>\n","      <td>0.124218</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Tenure</td>\n","      <td>0.068144</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>IsActiveMember</td>\n","      <td>0.059625</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Geography</td>\n","      <td>0.043623</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Gender</td>\n","      <td>0.018341</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>HasCrCard</td>\n","      <td>0.014974</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Feature Importance\n","3              Age   0.264004\n","6    NumOfProducts   0.147922\n","5          Balance   0.134915\n","9  EstimatedSalary   0.124233\n","0      CreditScore   0.124218\n","4           Tenure   0.068144\n","8   IsActiveMember   0.059625\n","1        Geography   0.043623\n","2           Gender   0.018341\n","7        HasCrCard   0.014974"]},"execution_count":322,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(data = np.c_[X_test.columns, feature_importances(best_forest)], columns = ['Feature', 'Importance']).sort_values(by='Importance', ascending=False)"]},{"cell_type":"code","execution_count":324,"metadata":{},"outputs":[{"data":{"text/plain":["(300, 51, 3)"]},"execution_count":324,"metadata":{},"output_type":"execute_result"}],"source":["best_params"]},{"cell_type":"markdown","metadata":{},"source":["Наиболее важным признаком оказался возраст клиента, наименее важными - пол клиента и наличие у него кредитной карты"]}],"metadata":{"colab":{"provenance":[{"file_id":"1eUpBIYzwvrMx-cgqAn-mNMPza9U8_gZW","timestamp":1698398808090},{"file_id":"1IAJXJC3FAdrLQnl2yVoEvX8jg27CzLm6","timestamp":1603282414216}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
