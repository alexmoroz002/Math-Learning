{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX9p5jfTySS"
      },
      "source": [
        "## Задание 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "Применим полученные навыки и решим задачу анализа тональности отзывов.\n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
<<<<<<< HEAD
        "Обязательные шаги предобработки:\n",
=======
        "Возможные шаги предобработки:\n",
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
<<<<<<< HEAD
        "6. построение модели\n",
        "7. оценка качества модели\n",
=======
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать гиперпараметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор.\n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
<<<<<<< HEAD
=======
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ],
      "source": [
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\moroz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\moroz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89995</th>\n",
              "      <td>сделано достаточно хорошо. на ткани сделан рис...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89996</th>\n",
              "      <td>Накидка шикарная. Спасибо большое провдо линяе...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89997</th>\n",
              "      <td>спасибо большое ) продовца рекомендую.. заказа...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89998</th>\n",
              "      <td>Очень довольна заказом! Меньше месяца в РБ.  К...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89999</th>\n",
              "      <td>хорошая куртка. постороннего запаха нет. швы р...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      качество плохое пошив ужасный (горловина напер...  negative\n",
              "1      Товар отдали другому человеку, я не получила п...  negative\n",
              "2      Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
              "3      товар не пришел, продавец продлил защиту без м...  negative\n",
              "4          Кофточка голая синтетика, носить не возможно.  negative\n",
              "...                                                  ...       ...\n",
              "89995  сделано достаточно хорошо. на ткани сделан рис...  positive\n",
              "89996  Накидка шикарная. Спасибо большое провдо линяе...  positive\n",
              "89997  спасибо большое ) продовца рекомендую.. заказа...  positive\n",
              "89998  Очень довольна заказом! Меньше месяца в РБ.  К...  positive\n",
              "89999  хорошая куртка. постороннего запаха нет. швы р...  positive\n",
              "\n",
              "[90000 rows x 2 columns]"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l_df = pd.read_csv('tables/women-clothing.csv', sep='\\t')\n",
        "l_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = l_df.copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.review, df.sentiment, train_size = 0.7, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_ngrams(ng_range):\n",
        "    vectorizer = CountVectorizer(ngram_range=ng_range)\n",
        "    X_train_vect = vectorizer.fit_transform(X_train)\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_vect, y_train)\n",
        "    X_test_vect = vectorizer.transform(X_test)\n",
        "    pred = clf.predict(X_test_vect)\n",
        "\n",
        "    return accuracy_score(y_test, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_ngrams():\n",
        "    best_acc = 0\n",
        "    best_range = (0, 0)\n",
        "    for i in range(1, 3):\n",
        "        for j in range(i, 5):\n",
        "            params = (i, j)\n",
        "            acc = test_ngrams(params)\n",
        "            print(f'params: {params}, score: {acc}')\n",
        "            if acc > best_acc:\n",
        "                best_range = params\n",
        "                best_acc = acc\n",
        "    return best_range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.7157512684715381\n",
            "params: (1, 2), score: 0.731417354912781\n",
            "params: (1, 3), score: 0.7320099255583127\n",
            "params: (1, 4), score: 0.7323062108810785\n",
            "params: (2, 2), score: 0.7178993370615903\n",
            "params: (2, 3), score: 0.716306803451724\n",
            "params: (2, 4), score: 0.7148994481685863\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ng_range, acc = get_best_ngrams()\n",
        "ng_range, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = df.copy()\n",
        "new_df.review = new_df.review.apply(lambda x: word_tokenize(x))\n",
        "new_df.review = new_df.review.apply(lambda x: ' '.join([word for word in x if word.isalpha()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.7156031258101552\n",
            "params: (1, 2), score: 0.7326024962038443\n",
            "params: (1, 3), score: 0.7336394948335246\n",
            "params: (1, 4), score: 0.7341209584830192\n",
            "params: (2, 2), score: 0.7174549090774416\n",
            "params: (2, 3), score: 0.7137883782082145\n",
            "params: (2, 4), score: 0.7122328802636939\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_df.review, new_df.sentiment, train_size = 0.7, random_state=17)\n",
        "get_best_ngrams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "После токенизации и удаления знаков препинания и чисел точность выросла, оставляем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = new_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Преобразование в нижний регистр"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = df.copy()\n",
        "new_df.review = new_df.review.apply(lambda x: word_tokenize(x))\n",
        "new_df.review = new_df.review.apply(lambda x: ' '.join([word.lower() for word in x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.7156031258101552\n",
            "params: (1, 2), score: 0.7326024962038443\n",
            "params: (1, 3), score: 0.7336394948335246\n",
            "params: (1, 4), score: 0.7341209584830192\n",
            "params: (2, 2), score: 0.7174549090774416\n",
            "params: (2, 3), score: 0.7137883782082145\n",
            "params: (2, 4), score: 0.7122328802636939\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_df.review, new_df.sentiment, train_size = 0.7, random_state=17)\n",
        "get_best_ngrams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Преобразование в нижний регистр очень слабо влияет на результат (в рамках погрешности). Но точность все равно повышается"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = new_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Удаление стоп-слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = df.copy()\n",
        "new_df.review = new_df.review.apply(lambda x: word_tokenize(x))\n",
        "new_df.review = new_df.review.apply(lambda x: ' '.join([word for word in x if word not in stopwords.words('russian')]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.7056034961668086\n",
            "params: (1, 2), score: 0.7156771971408467\n",
            "params: (1, 3), score: 0.7159734824636125\n",
            "params: (1, 4), score: 0.7159734824636125\n",
            "params: (2, 2), score: 0.6764934632050664\n",
            "params: (2, 3), score: 0.6713455057220103\n",
            "params: (2, 4), score: 0.6705307210844043\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_df.review, new_df.sentiment, train_size = 0.7, random_state=17)\n",
        "get_best_ngrams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Точность сильно снизилась, не используем"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = df.copy()\n",
        "new_df.review = new_df.review.apply(lambda x: word_tokenize(x))\n",
        "analyzer = MorphAnalyzer()\n",
        "new_df.review = new_df.review.apply(lambda x: ' '.join([analyzer.parse(word)[0].normal_form for word in x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.708418206733084\n",
            "params: (1, 2), score: 0.7299359282989519\n",
            "params: (1, 3), score: 0.7324173178771156\n",
            "params: (1, 4), score: 0.7323432465464242\n",
            "params: (2, 2), score: 0.7202325839783712\n",
            "params: (2, 3), score: 0.7191215140179993\n",
            "params: (2, 4), score: 0.7189733713566164\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "execution_count": 273,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_df.review, new_df.sentiment, train_size = 0.7, random_state=17)\n",
        "get_best_ngrams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Точность также снизилась, но значительно меньше, так что можно использовать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = new_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Итого, предобработка данного текста для повышения точности заключается в токенизации, удалении чисел и знаков препинания и преобразовании к нижнему регистру. Лемматизация и удаление стоп-слов не используются, так как снижают точность "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.7156031258101552\n",
            "params: (1, 2), score: 0.7326024962038443\n",
            "params: (1, 3), score: 0.7336394948335246\n",
            "params: (1, 4), score: 0.7341209584830192\n",
            "params: (2, 2), score: 0.7174549090774416\n",
            "params: (2, 3), score: 0.7137883782082145\n",
            "params: (2, 4), score: 0.7122328802636939\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 326,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = l_df.copy()\n",
        "df.review = df.review.apply(lambda x: word_tokenize(x))\n",
        "df.review = df.review.apply(lambda x: ' '.join([word.lower() for word in x if word.isalpha()]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.review, df.sentiment, train_size = 0.7, random_state=17)\n",
        "get_best_ngrams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "params: (1, 1), score: 0.7156031258101552\n",
            "params: (1, 2), score: 0.7326024962038443\n",
            "params: (1, 3), score: 0.7336394948335246\n",
            "params: (1, 4), score: 0.7341209584830192\n",
            "params: (2, 2), score: 0.7174549090774416\n",
            "params: (2, 3), score: 0.7137883782082145\n",
            "params: (2, 4), score: 0.7122328802636939\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7341209584830192"
            ]
          },
          "execution_count": 299,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ng_range = get_best_ngrams()\n",
        "test_ngrams(ng_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7341209584830192"
            ]
          },
          "execution_count": 328,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_pipe = Pipeline([\n",
        "    (\"vect\", CountVectorizer(ngram_range=ng_range)),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "word_pipe.fit(X_train, y_train)\n",
        "word_predict = word_pipe.predict(X_test)\n",
        "accuracy_score(y_test, word_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7254546127921188"
            ]
          },
          "execution_count": 329,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "\n",
        "tf_pipe = Pipeline([\n",
        "    (\"vect\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "tf_param = {\n",
        "    \"vect__max_df\": (0.5, 0.7, 0.9),\n",
        "    \"vect__min_df\": (0, 0.01),\n",
        "    \"vect__max_features\": (10000, 30000),\n",
        "    \"vect__ngram_range\": ((1,2), (1,3), (2,2))\n",
        "}\n",
        "\n",
        "tfidfcv = HalvingGridSearchCV(tf_pipe, tf_param, cv=2)\n",
        "\n",
        "tfidfcv.fit(X_train, y_train)\n",
        "tfidf_predict = tfidfcv.predict(X_test)\n",
        "accuracy_score(y_test, tfidf_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Char n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "6 fits failed out of a total of 18.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "2 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1370, in fit_transform\n",
            "    self._validate_ngram_range()\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 519, in _validate_ngram_range\n",
            "    raise ValueError(\n",
            "ValueError: Invalid value for ngram_range=(4, 3) lower boundary larger than the upper boundary.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "2 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1370, in fit_transform\n",
            "    self._validate_ngram_range()\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 519, in _validate_ngram_range\n",
            "    raise ValueError(\n",
            "ValueError: Invalid value for ngram_range=(5, 3) lower boundary larger than the upper boundary.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "2 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1370, in fit_transform\n",
            "    self._validate_ngram_range()\n",
            "  File \"c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 519, in _validate_ngram_range\n",
            "    raise ValueError(\n",
            "ValueError: Invalid value for ngram_range=(5, 4) lower boundary larger than the upper boundary.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.67490712 0.68490997 0.69305516        nan 0.69219777 0.69677051\n",
            "        nan        nan 0.69891398]\n",
            "  warnings.warn(\n",
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.74264075 0.79694198 0.84081166        nan 0.82037725 0.86038868\n",
            "        nan        nan 0.88268077]\n",
            "  warnings.warn(\n",
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.67490712 0.68490997 0.69305516        nan 0.69219777 0.69677051\n",
            "        nan        nan 0.69891398 0.69775195 0.70127643 0.70337207]\n",
            "  warnings.warn(\n",
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.74264075 0.79694198 0.84081166        nan 0.82037725 0.86038868\n",
            "        nan        nan 0.88268077 0.78448276 0.80124786 0.82172795]\n",
            "  warnings.warn(\n",
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.67490712 0.68490997 0.69305516        nan 0.69219777 0.69677051\n",
            "        nan        nan 0.69891398 0.69775195 0.70127643 0.70337207\n",
            " 0.71162089]\n",
            "  warnings.warn(\n",
            "c:\\Users\\moroz\\anaconda3\\envs\\py_311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.74264075 0.79694198 0.84081166        nan 0.82037725 0.86038868\n",
            "        nan        nan 0.88268077 0.78448276 0.80124786 0.82172795\n",
            " 0.77736149]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7151586978260065"
            ]
          },
          "execution_count": 330,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "char_pipe = Pipeline([\n",
        "    (\"vect\", CountVectorizer(analyzer='char')),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "char_param = {\n",
        "    \"vect__ngram_range\": list(itertools.product(range(3, 6), range(3, 6)))\n",
        "}\n",
        "\n",
        "charcv = HalvingGridSearchCV(char_pipe, char_param, cv=2)\n",
        "\n",
        "charcv.fit(X_train, y_train)\n",
        "char_predict = charcv.predict(X_test)\n",
        "accuracy_score(y_test, char_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-score</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Word n-gramms</th>\n",
              "      <td>0.736976</td>\n",
              "      <td>0.734343</td>\n",
              "      <td>0.735381</td>\n",
              "      <td>0.734121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tf-idf</th>\n",
              "      <td>0.735648</td>\n",
              "      <td>0.725813</td>\n",
              "      <td>0.727243</td>\n",
              "      <td>0.725455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Char n-gramms</th>\n",
              "      <td>0.723867</td>\n",
              "      <td>0.715532</td>\n",
              "      <td>0.716277</td>\n",
              "      <td>0.715159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Precision    Recall  F1-score  Accuracy\n",
              "Word n-gramms   0.736976  0.734343  0.735381  0.734121\n",
              "tf-idf          0.735648  0.725813  0.727243  0.725455\n",
              "Char n-gramms   0.723867  0.715532  0.716277  0.715159"
            ]
          },
          "execution_count": 347,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_df = pd.DataFrame(\n",
        "    data={\n",
        "        'Precision' : [precision_score(y_test, word_predict, average='macro'), precision_score(y_test, tfidf_predict, average='macro'), precision_score(y_test, char_predict, average='macro')],\n",
        "        'Recall' : [recall_score(y_test, word_predict, average='macro'), recall_score(y_test, tfidf_predict, average='macro'), recall_score(y_test, char_predict, average='macro')],\n",
        "        'F1-score' : [f1_score(y_test, word_predict, average='macro'), f1_score(y_test, tfidf_predict, average='macro'), f1_score(y_test, char_predict, average='macro')],\n",
        "        'Accuracy' : [accuracy_score(y_test, word_predict), accuracy_score(y_test, tfidf_predict), accuracy_score(y_test, char_predict)]\n",
        "    },\n",
        "    index=['Word n-gramms', 'tf-idf', 'Char n-gramms']\n",
        ")\n",
        "res_df"
      ]
    },
    {
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Задание 5.2 Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах.\n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1,
=======
      "execution_count": 188,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **а?** - ноль или один символ **а**\n",
        "* **а+** - один или более символов **а**\n",
        "* **а\\*** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 2,
=======
      "execution_count": 189,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Wh4EZcUVWB",
        "outputId": "eb3baf5e-4bf5-4e32-e9c0-b08fab66ba7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['abb', 'abb', 'bb']\n"
          ]
        }
      ],
      "source": [
        "result = re.findall('a?b.', 'aabbсabbcbb')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 3,
=======
      "execution_count": 190,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN7gjUwWGON",
        "outputId": "4c7da339-1c1d-464d-e958-18f091f3c1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aabb', 'abb', 'bb']\n"
          ]
        }
      ],
      "source": [
        "result = re.findall('a*b.', 'aabbсabbcbb')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 4,
=======
      "execution_count": 191,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izXJKMwJWKyY",
        "outputId": "358233f6-4a73-4e40-b797-b2d4aed3acd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aabb', 'abb']\n"
          ]
        }
      ],
      "source": [
        "result = re.findall('a+b.', 'aabbсabbcbb')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**:\n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 5,
=======
      "execution_count": 192,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2athHzKuWhAd",
        "outputId": "cafee36f-540d-4206-a320-d98ae94c4c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ],
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {
        "id": "7ZR2AEq3WhAg"
      },
      "outputs": [],
      "source": [
        "def first_2chars(string):\n",
        "    res = re.findall()"
=======
      "execution_count": 357,
      "metadata": {
        "id": "7ZR2AEq3WhAg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Lo', 'ip', 'do', 'si', 'am', 'co', 'ad', 'el', 'se', 'do', 'ei']"
            ]
          },
          "execution_count": 357,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.findall(r'\\b\\w{2}', \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod\")"
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 193,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVKdRoc1WhAl",
        "outputId": "78b5d289-8e8c-4621-fe3c-34aa130c727c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 194,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U9EQZMwWhAn",
        "outputId": "45699604-9950-4185-cd4f-0e3ad6655629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {
        "id": "dVgPSjEOWhAp"
      },
      "outputs": [],
      "source": []
=======
      "execution_count": 374,
      "metadata": {
        "id": "dVgPSjEOWhAp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Один', ' Два', ' Три. Четыре.']"
            ]
          },
          "execution_count": 374,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.split(r'\\.', 'Один. Два. Три. Четыре.', maxsplit=2)"
      ]
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 195,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az3KxKWwWhAr",
        "outputId": "93f260be-ce79-405c-b21a-5ca8345ff4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ]
        }
      ],
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {
        "id": "s_Sdu7xlWhAu"
      },
      "outputs": [],
      "source": []
=======
      "execution_count": 377,
      "metadata": {
        "id": "s_Sdu7xlWhAu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'DIG DIGDIGDIG DIGDIGDIG DIGDIG DIGDIG - проще позвонить чем у кого-то занимать'"
            ]
          },
          "execution_count": 377,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub(r'\\d', 'DIG', '8 800 555 35 35 - проще позвонить чем у кого-то занимать')"
      ]
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {
        "id": "KwNS9zt4WhAv"
      },
      "outputs": [],
      "source": []
=======
      "execution_count": 383,
      "metadata": {
        "id": "KwNS9zt4WhAv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Википедия  - свободная энциклопедия'"
            ]
          },
          "execution_count": 383,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub(r'http?\\S+', '', 'Википедия https://ru.wikipedia.org/ - свободная энциклопедия')"
      ]
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 196,
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JstTupisWhAy",
        "outputId": "f60e13a6-9d93-46e5-f329-620f6072715f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
<<<<<<< HEAD
          "execution_count": 147,
          "metadata": {
            "tags": []
          },
=======
          "execution_count": 196,
          "metadata": {},
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {
        "id": "nFvnIWbUWhA2"
      },
      "outputs": [],
      "source": []
=======
      "execution_count": 389,
      "metadata": {
        "id": "nFvnIWbUWhA2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Слова', 'больше', 'больше', 'слов']"
            ]
          },
          "execution_count": 389,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.compile(r'\\w{4,}').findall('Слова? Да, больше, ещё больше слов! Что-то ещё.')"
      ]
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
<<<<<<< HEAD
=======
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
            ]
          },
          "execution_count": 390,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.compile(r'@[\\w.]+').findall('abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')"
      ]
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zpq4QOU5Wg-H",
        "i_7DyyXRWg-K",
        "_JewKs4XU-so",
        "5yiLk1P_xYQ2",
        "VlWxW3e9Wg-m",
        "D39SSh0zWg-r",
        "rhVrgkSaWg_K",
        "XsRf9T_SWg_U",
        "ylKZG2MwWg_f",
        "9hedBdcYWhAH",
        "JrqW55jgWhAR",
        "5QYTwyMtWhAZ",
        "DbJrUpARWhAd",
        "MI18l-l9WhAk",
        "1wrEGqBSWhAr",
        "gStgBJy2WhAx"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
<<<<<<< HEAD
      "version": "3.10.11"
=======
      "version": "3.11.5"
>>>>>>> 8013e94c23782abfb678d9f38e31fcd87a3d40df
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
